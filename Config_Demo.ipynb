{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d26eeaa",
   "metadata": {},
   "source": [
    "# Configuration Management & Experiment Tracking Demo\n",
    "\n",
    "This notebook demonstrates the configuration management system and experiment tracking features.\n",
    "\n",
    "**Note:** This is optional reference material. The main project workflow is in `Project4-Semantic-Segmentation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Management & Experiment Tracking Setup\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION MANAGEMENT SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "This section demonstrates using YAML configs for easy model management\n",
    "and performance tracking across different experiments.\n",
    "\n",
    "Benefits:\n",
    "- Store different model configurations (baseline, large, fast, etc.)\n",
    "- Easy switching between configurations\n",
    "- Automatic performance logging\n",
    "- Track metrics and generate analysis reports\n",
    "\"\"\")\n",
    "\n",
    "# Create sample config files if they don't exist\n",
    "from src.config_loader import create_sample_configs, ConfigLoader, PerformanceTracker\n",
    "\n",
    "config_dir = create_sample_configs()\n",
    "print(f\"\\n✓ Sample configs created in: {config_dir}\")\n",
    "print(\"\\nAvailable configs:\")\n",
    "for cfg_file in sorted(config_dir.glob(\"*.yaml\")):\n",
    "    print(f\"  - {cfg_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Configuration from YAML\n",
    "\n",
    "# Choose which config to use\n",
    "config_name = \"config_baseline.yaml\"  # Change to other configs for experiments\n",
    "config_path = f\"./configs/{config_name}\"\n",
    "\n",
    "# Load configuration\n",
    "experiment_config = ConfigLoader.load_experiment_config(config_path)\n",
    "\n",
    "print(f\"Loaded Configuration: {config_name}\")\n",
    "print(f\"Experiment Name: {experiment_config.experiment_name}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Model: {experiment_config.model.model_name}\")\n",
    "print(f\"  Classes: {experiment_config.model.num_classes}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Learning Rate: {experiment_config.training.learning_rate}\")\n",
    "print(f\"  Max Iterations: {experiment_config.training.max_iter}\")\n",
    "print(f\"  Batch Size: {experiment_config.training.batch_size}\")\n",
    "print(f\"  Image Size: {experiment_config.data.image_size}\")\n",
    "\n",
    "# You can also create a custom config\n",
    "# experiment_config = ExperimentConfig(\n",
    "#     experiment_name=\"my_custom_experiment\",\n",
    "#     model=ModelConfig(...),\n",
    "#     training=TrainingConfig(...),\n",
    "#     data=DataConfig(...)\n",
    "# )\n",
    "\n",
    "print(\"\\n✓ Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Performance Tracker\n",
    "\n",
    "# Create a tracker for this experiment\n",
    "tracker = PerformanceTracker(\n",
    "    output_dir=experiment_config.output_dir,\n",
    "    experiment_name=experiment_config.experiment_name\n",
    ")\n",
    "\n",
    "print(f\"Performance Tracker initialized\")\n",
    "print(f\"Metrics will be saved to: {tracker.metrics_dir}\")\n",
    "print(f\"\\nTracker Features:\")\n",
    "print(\"  - Log metrics per iteration (epoch)\")\n",
    "print(\"  - Log metrics per batch (for training)\")\n",
    "print(\"  - Save results to JSON and CSV\")\n",
    "print(\"  - Generate summary report\")\n",
    "print(\"  - Plot metrics over training\")\n",
    "print(\"  - Find and report best metrics\")\n",
    "\n",
    "# Save the configuration to the output directory\n",
    "output_config_path = tracker.metrics_dir / \"config.yaml\"\n",
    "ConfigLoader.save_config(experiment_config, str(output_config_path))\n",
    "print(f\"\\n✓ Configuration saved to: {output_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ae467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Logging Metrics During Training\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE: Logging Metrics During Training\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Here's how to use the tracker during your training loop:\n",
    "\n",
    "During training, log metrics like this:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            loss = train_step(...)\n",
    "\n",
    "            # Log batch metrics (optional, for monitoring)\n",
    "            if batch_idx % 10 == 0:\n",
    "                tracker.log_batch(epoch, batch_idx, {'loss': loss})\n",
    "\n",
    "        # Validation phase\n",
    "        val_metrics = validate(model, val_loader)\n",
    "        tracker.log_iteration(epoch, val_metrics, stage='val')\n",
    "\n",
    "After training, save results:\n",
    "\n",
    "    tracker.save_metrics_json()   # Save to JSON\n",
    "    tracker.save_metrics_csv()    # Save to CSV\n",
    "    tracker.save_summary()        # Generate text summary\n",
    "    tracker.plot_metrics()        # Plot metrics\n",
    "\"\"\")\n",
    "\n",
    "# Example: Simulate logging some dummy metrics\n",
    "print(\"\\nSimulating a training run with dummy metrics...\")\n",
    "\n",
    "# Simulate 5 epochs\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Simulate training loss (decreasing)\n",
    "    train_loss = 1.0 - (epoch * 0.15) + np.random.normal(0, 0.05)\n",
    "    tracker.log_iteration(epoch, {'loss': train_loss}, stage='train')\n",
    "\n",
    "    # Simulate validation metrics (improving)\n",
    "    val_loss = 0.9 - (epoch * 0.12) + np.random.normal(0, 0.08)\n",
    "    val_iou = 0.3 + (epoch * 0.12) + np.random.normal(0, 0.02)\n",
    "    val_dice = 0.35 + (epoch * 0.13) + np.random.normal(0, 0.02)\n",
    "\n",
    "    tracker.log_iteration(\n",
    "        epoch,\n",
    "        {\n",
    "            'loss': val_loss,\n",
    "            'iou': val_iou,\n",
    "            'dice': val_dice,\n",
    "        },\n",
    "        stage='val'\n",
    "    )\n",
    "\n",
    "print(f\"✓ Logged {len(tracker.metrics_history)} metric records\")\n",
    "print(f\"\\nMetric history sample (last 3 records):\")\n",
    "for record in tracker.metrics_history[-3:]:\n",
    "    print(f\"  {record}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Metrics and Generate Analysis Reports\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING METRICS AND GENERATING REPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save metrics in different formats\n",
    "json_file = tracker.save_metrics_json()\n",
    "csv_file = tracker.save_metrics_csv()\n",
    "summary_file = tracker.save_summary()\n",
    "\n",
    "print(f\"\\n✓ Saved metrics to:\")\n",
    "print(f\"  JSON: {json_file}\")\n",
    "print(f\"  CSV:  {csv_file}\")\n",
    "print(f\"  Summary: {summary_file}\")\n",
    "\n",
    "# Read and display summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "with open(summary_file) as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Find best metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_val_iou = tracker.get_best_metrics('iou', stage='val')\n",
    "best_val_dice = tracker.get_best_metrics('dice', stage='val')\n",
    "\n",
    "if best_val_iou:\n",
    "    print(f\"\\nBest Validation IoU:\")\n",
    "    print(f\"  Iteration: {best_val_iou.get('iteration')}\")\n",
    "    print(f\"  IoU Score: {best_val_iou.get('iou'):.6f}\")\n",
    "\n",
    "if best_val_dice:\n",
    "    print(f\"\\nBest Validation Dice:\")\n",
    "    print(f\"  Iteration: {best_val_dice.get('iteration')}\")\n",
    "    print(f\"  Dice Score: {best_val_dice.get('dice'):.6f}\")\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Metrics Over Training\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VISUALIZATION: Metrics Over Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot selected metrics\n",
    "tracker.plot_metrics(metrics_to_plot=['loss', 'iou', 'dice'])\n",
    "\n",
    "print(\"\\n✓ Metrics plotted and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c05bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Comparison Workflow\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARING MULTIPLE EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Workflow for comparing different configurations:\n",
    "\n",
    "1. Run experiment with config_baseline.yaml\n",
    "   - Train model\n",
    "   - Track metrics with PerformanceTracker\n",
    "   - Save results\n",
    "\n",
    "2. Run experiment with config_large_model.yaml\n",
    "   - Same steps, different config\n",
    "   - Results saved in separate directory\n",
    "\n",
    "3. Compare Results:\n",
    "\"\"\")\n",
    "\n",
    "# Example comparison\n",
    "def compare_experiments(experiment_dirs):\n",
    "    \"\"\"Compare metrics across multiple experiments\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for exp_dir in experiment_dirs:\n",
    "        exp_path = Path(exp_dir)\n",
    "        metrics_files = list(exp_path.glob(\"metrics/*.json\"))\n",
    "\n",
    "        if metrics_files:\n",
    "            with open(metrics_files[0]) as f:\n",
    "                metrics = json.load(f)\n",
    "\n",
    "            # Extract best validation metrics\n",
    "            val_records = [m for m in metrics if m.get('stage') == 'val']\n",
    "            if val_records:\n",
    "                results[exp_path.name] = {\n",
    "                    'best_iou': max([m.get('iou', 0) for m in val_records]),\n",
    "                    'best_dice': max([m.get('dice', 0) for m in val_records]),\n",
    "                }\n",
    "\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Example: Create comparison table (with dummy data)\n",
    "comparison_data = {\n",
    "    'baseline': {'best_iou': 0.65, 'best_dice': 0.72},\n",
    "    'large_model': {'best_iou': 0.70, 'best_dice': 0.76},\n",
    "    'fast_train': {'best_iou': 0.58, 'best_dice': 0.64},\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "print(\"\\nExample Experiment Comparison:\")\n",
    "print(comparison_df)\n",
    "print(\"\\n✓ Use this workflow to track and compare all your experiments!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
